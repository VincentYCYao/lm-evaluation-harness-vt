{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-10T18:24:57.417901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-10:18:25:01,570 INFO     [__main__.py:209] Verbosity set to INFO\r\n",
      "2024-03-10:18:25:01,570 INFO     [__init__.py:358] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.\r\n",
      "2024-03-10:18:25:03,736 WARNING  [__main__.py:221]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\r\n",
      "2024-03-10:18:25:03,736 INFO     [__main__.py:226] Including path: ./probing_tasks_vt\r\n",
      "2024-03-10:18:25:03,736 INFO     [__init__.py:349] To still use tasks loaded from args.include_path,see an example of the new TaskManager API in https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage\r\n",
      "2024-03-10:18:25:03,736 INFO     [__main__.py:285] Selected Tasks: ['biolama_ctd']\r\n",
      "2024-03-10:18:25:03,736 INFO     [__main__.py:286] Loading selected tasks...\r\n",
      "2024-03-10:18:25:03,736 INFO     [evaluator.py:95] Setting random seed to 0\r\n",
      "2024-03-10:18:25:03,736 INFO     [evaluator.py:99] Setting numpy seed to 1234\r\n",
      "2024-03-10:18:25:03,736 INFO     [evaluator.py:103] Setting torch manual seed to 1234\r\n",
      "2024-03-10:18:25:03,748 INFO     [huggingface.py:178] Using `accelerate launch` or `parallelize=True`, device 'mps' will be overridden when placing model.\r\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!python lm_eval \\\n",
    "        --model hf \\\n",
    "        --model_args pretrained=meta-llama/Llama-2-13b-chat-hf,parallelize=True \\\n",
    "        --tasks biolama_ctd \\\n",
    "        --device mps \\\n",
    "        --batch_size 2 \\\n",
    "        --output_path /Users/vincent/Documents/GP-NLP/lm-eval-log/llama-2-13b-chat-biolama-ctd \\\n",
    "        --include_path ./probing_tasks_vt \\\n",
    "        --log_samples\\\n",
    "        --limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "62602beaf1f626fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
