{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-10T19:12:36.968029Z",
     "start_time": "2024-03-10T19:11:39.068647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-10:19:11:42,399 INFO     [__main__.py:209] Verbosity set to INFO\r\n",
      "2024-03-10:19:11:42,399 INFO     [__init__.py:358] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.\r\n",
      "2024-03-10:19:11:44,458 WARNING  [__main__.py:221]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\r\n",
      "2024-03-10:19:11:44,458 INFO     [__main__.py:226] Including path: ./probing_tasks_vt\r\n",
      "2024-03-10:19:11:44,458 INFO     [__init__.py:349] To still use tasks loaded from args.include_path,see an example of the new TaskManager API in https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage\r\n",
      "2024-03-10:19:11:44,458 INFO     [__main__.py:285] Selected Tasks: ['biolama_ctd']\r\n",
      "2024-03-10:19:11:44,458 INFO     [__main__.py:286] Loading selected tasks...\r\n",
      "2024-03-10:19:11:44,458 INFO     [evaluator.py:95] Setting random seed to 0\r\n",
      "2024-03-10:19:11:44,458 INFO     [evaluator.py:99] Setting numpy seed to 1234\r\n",
      "2024-03-10:19:11:44,458 INFO     [evaluator.py:103] Setting torch manual seed to 1234\r\n",
      "2024-03-10:19:11:44,459 WARNING  [evaluator.py:114] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\r\n",
      "2024-03-10:19:11:44,469 INFO     [huggingface.py:178] Using `accelerate launch` or `parallelize=True`, device 'mps' will be overridden when placing model.\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:45<00:00, 15.07s/it]\r\n",
      "Using cache at True_rank0.db\r\n",
      "2024-03-10:19:12:31,473 INFO     [evaluator.py:150] get_task_dict has been updated to accept an optional argument, `task_manager`Read more here:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage\r\n",
      "2024-03-10:19:12:33,644 WARNING  [task.py:308] [Task: None] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\r\n",
      "2024-03-10:19:12:33,644 WARNING  [task.py:308] [Task: None] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\r\n",
      "2024-03-10:19:12:35,218 INFO     [task.py:361] Building contexts for None on rank 0...\r\n",
      "2024-03-10:19:12:35,218 INFO     [evaluator.py:369] Running generate_until requests\r\n",
      "2024-03-10:19:12:35,218 INFO     [model.py:203] Loading 'generate_until' responses from cache 'True_rank0.db' where possible...\r\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1412.22it/s]\r\n",
      "0it [00:00, ?it/s]\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "hf (pretrained=meta-llama/Llama-2-13b-chat-hf,parallelize=True), gen_kwargs: (temperature=0), limit: 1.0, num_fewshot: None, batch_size: 1\r\n",
      "|   Tasks   |Version|Filter|n-shot| Metric |Value|   |Stderr|\r\n",
      "|-----------|------:|------|------|--------|----:|---|------|\r\n",
      "|biolama_ctd|      0|none  |None  |topk_acc|    0|±  |N/A   |\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python lm_eval \\\n",
    "        --model hf \\\n",
    "        --model_args pretrained=meta-llama/Llama-2-13b-chat-hf,parallelize=True \\\n",
    "        --tasks biolama_ctd \\\n",
    "        --device mps \\\n",
    "        --batch_size 1 \\\n",
    "        --output_path /Users/vincent/Documents/GP-NLP/lm-eval-log/llama-2-13b-chat-biolama-ctd \\\n",
    "        --include_path ./probing_tasks_vt \\\n",
    "        --log_samples\\\n",
    "        --limit 1 \\\n",
    "        --use_cache True \\\n",
    "        --gen_kwargs temperature=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-10:19:08:36,160 INFO     [__main__.py:209] Verbosity set to INFO\r\n",
      "2024-03-10:19:08:36,160 INFO     [__init__.py:358] lm_eval.tasks.initialize_tasks() is deprecated and no longer necessary. It will be removed in v0.4.2 release. TaskManager will instead be used.\r\n",
      "2024-03-10:19:08:38,343 WARNING  [__main__.py:221]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\r\n",
      "2024-03-10:19:08:38,343 INFO     [__main__.py:226] Including path: ./probing_tasks_vt\r\n",
      "2024-03-10:19:08:38,343 INFO     [__init__.py:349] To still use tasks loaded from args.include_path,see an example of the new TaskManager API in https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage\r\n",
      "2024-03-10:19:08:38,344 INFO     [__main__.py:285] Selected Tasks: ['biolama_ctd']\r\n",
      "2024-03-10:19:08:38,344 INFO     [__main__.py:286] Loading selected tasks...\r\n",
      "2024-03-10:19:08:38,344 INFO     [evaluator.py:95] Setting random seed to 0\r\n",
      "2024-03-10:19:08:38,344 INFO     [evaluator.py:99] Setting numpy seed to 1234\r\n",
      "2024-03-10:19:08:38,344 INFO     [evaluator.py:103] Setting torch manual seed to 1234\r\n",
      "2024-03-10:19:08:38,346 WARNING  [evaluator.py:114] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\r\n",
      "2024-03-10:19:08:38,356 INFO     [huggingface.py:178] Using `accelerate launch` or `parallelize=True`, device 'mps' will be overridden when placing model.\r\n",
      "config.json: 100%|█████████████████████████████| 595/595 [00:00<00:00, 1.28MB/s]\r\n",
      "model.safetensors: 100%|███████████████████| 53.3M/53.3M [00:01<00:00, 29.3MB/s]\r\n",
      "generation_config.json: 100%|███████████████████| 111/111 [00:00<00:00, 631kB/s]\r\n",
      "tokenizer_config.json: 100%|████████████████████| 264/264 [00:00<00:00, 795kB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:00<00:00, 5.15MB/s]\r\n",
      "special_tokens_map.json: 100%|████████████████| 99.0/99.0 [00:00<00:00, 331kB/s]\r\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "Using cache at True_rank0.db\r\n",
      "2024-03-10:19:08:43,665 INFO     [evaluator.py:150] get_task_dict has been updated to accept an optional argument, `task_manager`Read more here:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage\r\n",
      "2024-03-10:19:08:46,055 WARNING  [task.py:308] [Task: None] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\r\n",
      "2024-03-10:19:08:46,055 WARNING  [task.py:308] [Task: None] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\r\n",
      "2024-03-10:19:08:47,535 INFO     [task.py:361] Building contexts for None on rank 0...\r\n",
      "2024-03-10:19:08:47,536 INFO     [evaluator.py:369] Running generate_until requests\r\n",
      "2024-03-10:19:08:47,536 INFO     [model.py:203] Loading 'generate_until' responses from cache 'True_rank0.db' where possible...\r\n",
      "100%|█████████████████████████████████████████| 10/10 [00:00<00:00, 9372.75it/s]\r\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:56<00:00,  7.07s/it]\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "Invalid JSON encountered.\r\n",
      "topk_dicts:  {'top_1': '', 'top_2': '', 'top_3': '', 'top_4': '', 'top_5': ''}\r\n",
      "hf (pretrained=EleutherAI/pythia-14m,parallelize=True), gen_kwargs: (temperature=0), limit: 10.0, num_fewshot: None, batch_size: 2\r\n",
      "|   Tasks   |Version|Filter|n-shot| Metric |Value|   |Stderr|\r\n",
      "|-----------|------:|------|------|--------|----:|---|-----:|\r\n",
      "|biolama_ctd|      0|none  |None  |topk_acc|    0|±  |     0|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python lm_eval \\\n",
    "        --model hf \\\n",
    "        --model_args pretrained=EleutherAI/pythia-14m,parallelize=True \\\n",
    "        --tasks biolama_ctd \\\n",
    "        --device mps \\\n",
    "        --batch_size 2 \\\n",
    "        --output_path /Users/vincent/Documents/GP-NLP/lm-eval-log/pythia-14m-biolama-ctd \\\n",
    "        --include_path ./probing_tasks_vt \\\n",
    "        --log_samples\\\n",
    "        --limit 10 \\\n",
    "        --use_cache True \\\n",
    "        --gen_kwargs temperature=0\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T19:09:44.934095Z",
     "start_time": "2024-03-10T19:08:32.020418Z"
    }
   },
   "id": "62602beaf1f626fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    " \"Consider the following sentence: \\\"Bacterial Toxins prevents diseases such as <BLANK>.\\\"\\n\\n-> Which noun-phrase should <BLANK> be filled with? Give me 5 most probable candidates. Output your response in JSON format with keys \\\"top_1\\\", \\\"top_2\\\", \\\"top_3\\\", \\\"top_4\\\" and \\\"top_5\\\", where the value for key \\\"top_1\\\" is the most promising entity that would replace <BLANK>."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a03862a1560cec5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11f83c1b6fc546e6822faa10208d0af5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'LlamaForCausalLM' is not supported for question-answering. Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FalconForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering', 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MptForQuestionAnswering', 'MraForQuestionAnswering', 'MT5ForQuestionAnswering', 'MvpForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'T5ForQuestionAnswering', 'UMT5ForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/6.02k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e454c89d631241308c533559e99850c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/6.02k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bb334ee74f541c4acf5a606ec253d08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/6.02k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "350815b2bde14a9db6215ca6d62c8558"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fbcebe8101c4cca89da202151180b94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28a2e598c1cf4efc92f2960bb6d44299"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "325f4c60da1a483dbab6d3166657efd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load the question answering pipeline with the desired model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "\n",
    "# Load the evaluation dataset\n",
    "dataset = load_dataset(\"CamWheeler135/Bio_Lama_TinyUMLS\")\n",
    "\n",
    "question = \"Consider the following sentence: \\\"Bacterial Toxins prevents diseases such as <BLANK>.\\\"\\n\\n-> Which noun-phrase should <BLANK> be filled with? Give me 5 most probable candidates. Output your response in JSON format with keys \\\"top_1\\\", \\\"top_2\\\", \\\"top_3\\\", \\\"top_4\\\" and \\\"top_5\\\", where the value for key \\\"top_1\\\" is the most promising entity that would replace <BLANK>\"\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_qa(examples):\n",
    "    predictions = qa_pipeline(\n",
    "        questions=question,\n",
    "        top_k=5,  # Specify the value of k for top-k accuracy\n",
    "    )\n",
    "\n",
    "    # Extract the predicted nouns from the model's output\n",
    "    predicted_nouns = [pred[\"answer\"].lower() for pred in predictions]\n",
    "\n",
    "    # Compare the predicted nouns with the ground truth answers\n",
    "    accuracy = sum(1 for pred, gold in zip(predicted_nouns, examples[\"answers\"]) if pred in gold) / len(examples[\"answers\"])\n",
    "\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Specify the value of k for top-k accuracy\n",
    "k = 5\n",
    "\n",
    "# Evaluate the model on the dataset\n",
    "results = dataset.map(evaluate_qa, batched=True, batch_size=16)\n",
    "\n",
    "# Print the overall top-k accuracy\n",
    "print(f\"Top-{k} Accuracy: {results['accuracy'].mean():.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-10T19:27:17.357759Z"
    }
   },
   "id": "c54ffd6551d766c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "792c1c1e6c90c27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7acb51d071174a5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dfbe412cf8952db"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2dbe91fe10fdf690"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8f43a60338ceb988"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "28845bdda1afcb46"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "67f2970ffdf695a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2850983c0652f90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6c708c948fc60f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
